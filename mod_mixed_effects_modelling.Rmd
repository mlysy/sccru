---
output:
  html_document: default
  pdf_document: default
---
# Introduction to Longitudinal Data and Mixed Effect Modelling 

*Author: Grace Tompkins*

*Last Updated: October 12, 2021*

--- 

## Introduction

```{r mem-setup-for-local, echo = F, warning = F, message = F}
source("common_functions.R")
require(bookdown)
```

To study the relationship between an outcome and other covariates, there are many types of studies and data that one can use. Although observational data is typically the easiest to obtain and analyze in practice, it can be limited with the types of conclusions we can draw from it due to the data being captured at only a single "snapshot" in time. While longitudinal studies can either be prospective (subjects are followed forward in time) or retrospective (measurements on subjects are extracted historically), prospective studies tend to preferred as recall bias, where subjects inaccurately remember past events, can impact the data collected [@diggle02].

We often turn to longitudinal studies in which we take repeated measurements from the individuals whom are followed over a certain period of time. The major advantage of longitudinal studies is that one can distinguish between changes of an outcome within an subject overtime (longitudinal effect )and differences among subjects at a given point in time (cohort effect). Longitudinal studies can also separate time effects and quantify different sources of variation in the data by separating the between-subject and within-subject variation. Observational studies do not have these benefits.

A challenge of longitudinal data is that observations taken within each subject are correlated. Even after a great amount of time separation between observations, the correlation between a pair of responses on the same unit rarely approaches zero [@fitzmaurice11]. We refer to this as the intra-subject correlation. This implies that our typical statistical modeling tools which assume independence among observations are inappropriate for this type of data. Methods that account for intra-subject correlation will be discussed further in the following sections. 

While longitudinal analysis is often used in the context of health data involving repeated measurements from patients, longitudinal data can be found in a variety of disciplines, including (but not limited to) [economics](https://www150.statcan.gc.ca/n1/en/pub/11f0019m/11f0019m2004227-eng.pdf?st=aVJbOo9q), [finance](https://www.sciencedirect.com/science/article/pii/S0167487018301648?casa_token=Zu4t8ubyZPAAAAAA:geM4RGeihOj0gGWgPeZ_xnNBB2ZA3nnOeYzlvSHHuCnEbbd9mmi-5iuR7ysEh_6Zm5_uaatnhKB_), [environmental studies](https://www.mdpi.com/2071-1050/9/6/913), and [education](https://www.tandfonline.com/doi/abs/10.1080/00220973.1997.9943456?casa_token=KBEjZbk9xRAAAAAA:INEZk1fuIr0H59P9fl0ykz5qWKxzXnR1PZ_6H0MHwvgCnjXWC8D0A5xbLadXutbsUJx3lvKF1yXj5QE). 


The methodology presented in the following sections is based on the [generalized linear model](https://en.wikipedia.org/wiki/Generalized_linear_model) framework. This framework is flexible and allows for the estimation of longitudinal data with different types of outcomes and covariates. 

### List of R packages Used {#mem-rpackages}

In this chapter, we will be using the packages `r cran_link("geesmv")`, `r cran_link("nmle")`and ,...


```{r mem-library, warning=FALSE, message=FALSE}
library(geesmv) # load the required packages
library(nlme)
```

### Motivating Example {#mem-motivating}

Throughout this chapter, we will be considering the data set `dental` from the R package `r cran_link("geesmv")`. 

We can first load the data set `dental` to the working environment.
```{r mem-data-load, warning=FALSE}
data("dental") # load the data dental from the geesmv package
```


This data set was obtained to study the growth of 27 children (16 boys and 11 girls). This data set contains orthodontic measurements (the distance from the center of the pituitary to the pterygomaxillary fissure) in millimeters, along with the children's genders. Measurements were taken from children at ages 8 (baseline), 10, 12, and 14. 

To look at the form of the data, we can look at the first six observations by: 

```{r mem-data-view, warning=FALSE}
head(dental) # look at the first 6 observations in the data set
```

In this data set, the `subject` variable identifies the child and the `gender` variable is a binary variable such that `gender = F` when the subject is female and `gender = M` if male. The last four columns show the orthodontic measurements for each child at the given age, which are continuous measurements.

Using this data, we want to ask the following questions:

* Does the orthodontic measurement increase as the age of the subjects increases?
* Is there a difference between growth in the two gender groups considered?

In order to answer these, we need to employ longitudinal methods which account for individual random effects and population effects, as described in the following sections. 

## Data Structure for Longitudinal Responses {#mem-datastruc}

Longitudinal data can be presented or stored in two different ways. *Wide form* data has a single row for each subject and a unique column for the response of the subject at different time points. In its unaltered form, the `dental` data set is in wide form. However, we often need to convert our data into *long form* in order to use many popular software packages for longitudinal data analysis. In long form, we have multiple rows per subject representing the outcome measured at different time points. We also include an additional variable denoting the time or occasion in which we obtained the measurement.

As an example, let's change the `dental` data set into long form. We can do this by employing the `reshape` function in `R`. The `reshape` function has many arguments available which can be explored by typing `?reshape` in the console. Some of the important arguments include:

* `data`: the dataframe we are converting.
* `direction`: the direction in which we are converting to.
* `idvar`: the column name of the variable identifying subjects (typically some type of id, or name)
* `varying`: the name of the sets of variables in the wide format that we want to transform into a single variable in long format ("time-varying"). Typically these are the column names of wide form data set in which the the repeated outcome is measured.
* `times`: the values we are going to use in the long form that indicate when the observations were taken
* `timevar`: the name of the variable in long form indicating the time. 
* `drop`: a vector of column names that we do not want to include in the newly reshaped data set.

To reshape the wide form dental data set into long form, we can execute the following code:

```{r mem-reshape}

#reshape the data into long form
dental.long <- reshape(data = dental, #original data in wide form 
                       direction = "long", #changing from wide TO long
                       idvar = "subject", #name of variable indicating unique 
                                          #subjects in wide form dataset
                       varying = c("age_8", "age_10", "age_12", "age_14"), #name 
                                        #of variables in which outcomes recorded
                       v.names = "distance", #assigning a new name to the outcome
                       times = c(8,10,12,14), #time points in which above 
                                              #outcomes were recorded
                       timevar = "age") #name of the time variable we're using

#order the data by subject ID and then by age
dental.long <- dental.long[order(dental.long$subject, dental.long$age), ] 


#look at the first 10 observations
head(dental.long, 10)

```

We see that the `distance` variable corresponds to one of the values in one of the last four columns of the `dental` data set in wide form for each subject. For the rest of the example, we will be using the data stored in `dental.long`. 




## Linear Models for Continuous Outcome {#mem-linear}

When we are analyzing data that has a continuous outcome, we can look at answering our research questions based simple linear model.  In this setting, we are assuming that the data has a *balanced design*, meaning that the observation times are the same for all subjects (and we also assume we have no missing data).

### Notation {#mem-linear-notation}

Assume we have $n$ individuals observed at $k$ different times. 

We let:

* $t_j$ for $j = 1, .., k$ be the $k$ common assessment times,
* $Y_{ij}$ for $i = 1, ... ,n$ and $j = 1, ... , k$ be the response of subject $i$ at assessment time $j$, and
* $x_{ij}$ for $i = 1, ... ,n$ and $j = 1, ... , k$ be a $p \times 1$ vector recording other covariates for subject $i$ at time $j$.

We can write the observed data at each time point (which are assumed to be common for all individuals in this setting) in matrix form. For each subject $i = 1, ..., n$, we let

$$
\bf{Y}_i = \begin{bmatrix}
Y_{i1} \\
Y_{i2} \\
\vdots \\
Y_{ik} \\
\end{bmatrix} , \text{     and } \bf{X}_i = \begin{bmatrix}
\bf{x}_{i1}^T \\
\bf{x}_{i2}^T  \\
\vdots \\
\bf{x}_{ik}^T \\
\end{bmatrix} = \begin{bmatrix}
x_{i11} & x_{i12}&\dots & x_{i1p} \\
x_{i21} & x_{i22}&\dots & x_{i2p} \\
\vdots & \vdots &\vdots & \vdots \\
x_{ik1} & x_{ik2}&\dots & x_{ikp} 
\end{bmatrix}.
$$

### Model Specification  {#mem-linear-modelspec}

We must specify the model for the mean of our outcome $Y$. We assume that $\bf{Y}_i$ conditional on $\bf{X}_i$ follows a multivariate distribution, where

$$
\bf{Y}_i | \bf{X}_i \sim N(\mu_i, \bf{\Sigma_i}).
$$
We specify a linear model for the $\mu_i$, the mean of the outcome of $Y$ conditional on $X$ as

$$
\mu_i = E(\bf{Y}_i | \bf{X}_i) = \bf{X}_i\beta = \begin{bmatrix}
\bf{x}_{i1}^T\beta \\
\bf{x}_{i2}^T\beta \\
\vdots \\
\bf{x}_{ik}^T\beta  \\
\end{bmatrix}.
$$

and also must specify the structure of the $n \times n$ covariance matrix $\bf{\Sigma}_i$. Unlike in most cross-sectional studies, longitudinal data is correlated due to the repeated samples taken within subjects. Thus, we need to model both the relationship between the outcome and covariates and the *autocorrelation*  from responses within an individual. 

For clarity, we can re-write the multivariate normal assumption as 

$$
\bf{Y}_i \sim N(\bf{X}_i\beta, \Sigma_i(\theta)).
$$
Without accounting for the autocorrelation, we may end up with

* incorrect conclusions and incorrect inferences on $\bf{\beta}$
* inefficient estimated of $\bf{\beta}$
* more biases caused by missing data

[@diggle02]. We also note that under a balanced design with common observation times, we assume a common covariance matrix for all individuals, that is

$$
\bf{\Sigma}_i = \begin{bmatrix}
\sigma_1^2 & \sigma_{12}& \dots & \sigma_{1k} \\
 & \sigma_2^2 & \dots & \sigma_{2k} \\
 &  & \ddots & \vdots\\
& &  & \sigma_k^2
\end{bmatrix}.
$$

The diagonal elements represent the variances of the outcome $Y$ while the off-diagonal elements represent the covariances between outcomes within a given individual. We consider different structures of covariance matrices as using this covariance matrix can be problematic due to the large number of parameters we need to estimate in practice. We refer to the collection of parameters in this variance-covariance matrix as $\theta = (\sigma_1^2, \sigma_2^2, ..., \sigma_k^2, \sigma_{12}, \sigma_{13}, ..., \sigma_{k-1,k})^T$.

We typically assume that the variances of the responses do not change overtime, and thus we can write 

$$
\bf{\Sigma}_i = \sigma^2\bf{R}_i
$$
where $\bf{R}_i$ is referred to as a correlation matrix such that

$$
\bf{R}_i = \begin{bmatrix}
1 & \rho_{12}& \dots & \rho_{1k} \\
 & 1 & \dots & \rho_{2k} \\
 &  & \ddots & \vdots\\
& &  & 1
\end{bmatrix}.
$$

This comes from the equation relating correlation and covariance as $\sigma_{12} = \sigma^2\rho_{12}$ when common variances are assumed. 

We consider different structures of $\bf{R}_i$ in our analyses and choose the most appropriate one based on the data. Commonly used correlation structures are:


* *Unstructured Correlation*, the least constrained structure

$$
\bf{R}_i = \begin{bmatrix}
1 & \rho_{12}& \dots & \rho_{1k} \\
 & 1 & \dots & \rho_{2k} \\
 &  & \ddots & \vdots\\
& &  & 1
\end{bmatrix},
$$


* *Exchangeable Correlation*, which is the simplest with only one parameter (exculding the variance $\sigma^2$) to estimate

$$
\bf{R}_i = \begin{bmatrix}
1 & \rho& \dots & \rho \\
 & 1 & \dots & \rho \\
 &  & \ddots & \vdots\\
& &  & 1
\end{bmatrix},
$$

* *First-order Auto Regressive Correlation*, which is sometimes referred to as "AR(1)" and is most suitable for evenly spaced observations where we see weaker correlation as the time between observations gets larger

$$
\bf{R}_i = \begin{bmatrix}
1 & \rho & \rho^2 &\dots & \rho^{k-1} \\
 & 1 & \rho &\dots  & \rho^{k-2} \\
 &  &  &\ddots & \vdots\\
& & & & 1
\end{bmatrix},
$$
and 

* *Exponential Correlation*, which collapses to AR(1) if observations are equally spaced where $\rho_{jl} = \exp(-\phi|t_{ij} - t_{il}|)$ for some $\phi > 0$

We note that in practice, it is possible that the variance-covariance matrices can differ among subjects, and the matrix may also depend on the covariates present in the data. More details about how to choose the appropriate structure will be discussed in the following sections. 

### Estimation {#mem-linear-estimation}

For convenience, let's condense our notation to stack the response vectors and rewrite the linear model as $\bf{Y}\sim N(\bf{X} \bf{\beta}, \Sigma)$ where


$$
\bf{Y} = \begin{bmatrix}
\bf{Y}_1 \\
\bf{Y}_2 \\
\vdots \\
\bf{Y}_n\\
\end{bmatrix} ,    \bf{X} = \begin{bmatrix}
\bf{X}_1 \\
\bf{X}_2  \\
\vdots \\
\bf{X}_n\\
\end{bmatrix}, \text{     and } \bf{\Sigma} = \begin{bmatrix}
\bf{\Sigma}_1 & 0 &\dots & 0 \\
 &  \bf{\Sigma}_2 &\dots  & 0 \\
 &    &\ddots & \vdots\\
& & & \bf{\Sigma}_n
\end{bmatrix},
$$


Under the multivariate normality assumptions, and with a fully specified distribution, one approach to estimating our regression parameters $\beta$ and variance-covariance parameters $\theta = (\sigma_1^2, \sigma_2^2, ..., \sigma_k^2, \sigma_{12}, \sigma_{13}, ..., \sigma_{k-1,k})^T$ is through maximum likelihood estimation. 

The maximum likelihood estimate (MLE) of $\beta$ is 

$$
\widehat{\bf{\beta}} = (\bf{X}^T\bf{\Sigma}^{-1}\bf{X})^{-1}\bf{X}^T\bf{\Sigma}^{-1}\bf{Y}
$$

which we note is a function of our variance-covariance matrix $\bf{\Sigma}$ and thus a function of the parameters $\theta$. As such, we can either estimate the parameters using [profile likelihood](https://en.wikipedia.org/wiki/Likelihood_function) or [restricted maximum likelihood estimation (REML)](https://en.wikipedia.org/wiki/Restricted_maximum_likelihood). Using profile likelihood estimation is desirable because of the MLE's large sample properties, however the MLEs of our variance and covariance parameters $\theta$ will be biased. REML was developed to overcome this issue. In general, the MLE and REML estimates are not equal to each other for the regression parameters $\beta$, and thus we typically only use REML when estimating the variance and covariance parameters. 

The MLE $\widehat\beta$ found my maximum likelihood estimation has asymptotic normality. That is,

$$
\hat{\beta} \sim N(\beta, [X^T\Sigma^{-1}X]^{-1}).
$$

As $\Sigma$ must be estimated, we typically estimate the asymptotic variance-covariance matrix as 

$$
\widehat{asvar}(\widehat{\beta}) = (X^T\widehat{\Sigma}^{-1}X)^{-1}
$$
We can use this to make inference about regression parameters and perform hypothesis testing. For example, 

$$
\frac{\widehat{\beta}_j - \beta_j}{\sqrt{\widehat{se}(\widehat{\beta}}_j)} \dot{\sim} N(0,1).
$$


### Modelling in R {#mem-linear-R}

To fit a linear model in `R`, we can use the `gls()` function from the `r cran_link("nmle")` package. This function has a number of parameters, including

* `model`: a linear formula description of the form `model = response ~ covariate1 + covariate2`. Interaction effects can be specified using the form `covariate1*covariate2`. 
* `correlation`: the name of the within-group correlation structure, which may include `corAR1` for the AR(1) structure, `corCompSymm` for the exchangeable structure, `corExp` for exponential structure, `corSymm` for unstructured, and others (see `?corClasses` for other options). Default is an independent covariance structure. 
* `weights`: an optional argument to allow for different marginal variances. For example, to allow for the variance of the responses to change for different discrete time/observation points, we can use `weights = varIndent(form ~1 | factor(time))`.
* `method`: the name of the estimation method, where options include "ML" and "REML" (default). 

To demonstrate the use of this package, we will apply the `gls()` function to the transformed (long form) dental data set. We first assess the assumption of normality in the outcome. We can the outcome for all subjects, using a histogram and a [quantile-quantile (QQ)](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) plot to assess normality.

```{r mem-normality}
par(mfrow = c(1,2)) #set graphs to be arranged in one row and two columns

hist(dental.long$distance, xlab = "Distance", main = "Histogram of Outcome")
#histogram of outcome

qqnorm(dental.long$distance) #plot quantiles against normal distribution
qqline(dental.long$distance) #add line
```

From these plots, we see that our outcome appears to be normal by the histogram and that there are no indications of non-normality in the data by the Q-Q plot. 

Now, we can start to build our model, treating `age` as a categorical time variable. let's define

* $z_i$ to be the indicator for if subject $i$ is male
* $t_{ij1}$ be the indicator for if the age of individual $i$ at observation $j$ is 10
* $t_{ij2}$ be the indicator for if the age of individual $i$ at observation $j$ is 12
* $t_{ij3}$ be the indicator for if the age of individual $i$ at observation $j$ is 14


The main effects model can be written as

$$
\mu_{ij} = \beta_0 + \beta_1z_i + \beta_2t_{ij1} + \beta_3t_{ij2} + \beta_4t_{ij3}.
$$

We can start by first assuming an independent working correlation model. In the next section, we will describe how to choose the working correlation structure.

To fit a model and see the output, we can write:

```{r mem-modelbuild1}
fit1 <- gls(distance ~ factor(gender)+factor(age), data = dental.long, 
            method = "ML") #no specified correlation structure means we are 
        #assuming independence. Note we are fitting the model using Maximum 
        #likelihood estimation and with no interactions (main effects only). 

summary(fit1) #see the output of the fit
```

Under the assumption that the working correlation is independent, we can assess which variables impact the outcome of our distance measurement. In the summary of the coefficients, we have very small p-values for our gender variable, indicating that there is a difference in distance measurements between boys and girls enrolled in the study. Additionally, the variables indicating the subject was at age 12 or 14 are very small, with increasingly large coefficients, providing evidence of a possible time trend in our data. These p-values come from a t-test for the null hypothesis that the coefficient of interest is non-zero. 


### Hypothesis Testing {#mem-linear-HT}




Suppose we want to see if the time trend differs between boys and girls enrolled in the study. To formally test if there is a common time-trend between groups (gender), we can fit a model including an interaction term and perform a hypothesis test. 

The interaction model can be written as

$$
\mu_{ij} = \beta_0 + \beta_1z_i + \beta_2t_{ij1} + \beta_3t_{ij2} + \beta_4t_{ij3} + \beta_5z_it_{ij1} + \beta_6z_it_{ij2} + \beta_7z_it_{ij3}.
$$
We fit the model as:

```{r mem-modelbuild2}
fit2 <- gls(distance ~ factor(gender)*factor(age), data = dental.long, 
            method = "ML") #no specified correlation structure means we are 
        #assuming independence. Note we are fitting the model using Maximum 
        #likelihood estimation and with interactions (by using *). 

summary(fit2) #see the output of the fit
```

We want to test if the last three coefficients ($\beta_5$, $\beta_6$, and $\beta_7$) are all equal to zero. We are testing $H_0: \beta_5 = \beta_6 = \beta_7 = 0$ vs $H_a$: at least one of these coefficients is non-zero. To do so, we need to define a matrix. This matrix has one column for each estimated coefficient (including the intercept) and one row for each coefficient in the hypothesis test. As such, for this particular hypothesis test let's define the matrix

$$
L = \begin{bmatrix}
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
$$
which will be used to calculate our Wald test-statistic $(L\widehat{\beta})^T[L\widehat{asvar})\widehat{\beta}L^T]^{-1}(L\widehat{\beta})$ which follows a chi-squared distribution with degrees of freedom equal to the rank of the matrix $L$ (which is 3 in this case). 

To perform this hypothesis test in `R`, we do the following:

```{r mem-waldtest1}
L <- rbind(c(0,0,0,0,0,1,0,0),
           c(0,0,0,0,0,0,1,0),
           c(0,0,0,0,0,0,0,1)) #create L matrix as above

betahat <- fit2$coef #get estimated beta hats from the model

asvar <- fit2$varBeta #get the estimated covariances from the model

#calculate test statistic using given formula
waldtest.stat <- t(L%*%betahat)%*%solve(L %*%asvar%*%t(L))%*%(L%*%betahat) 
waldtest.stat
```

To get a p-value for this test, we perform the following:
```{r mem-pvalue1}
p.val <- 1 - pchisq(waldtest.stat, 3) #test stat follows chi-squared 3 in this case
p.val
```

We have a large p-value, which tells us that we do not have sufficient evidence against $H_0$. That is, we do not have evidence to suggest that the time trends vary by gender, and the model (`fit1`) without the interaction is most sufficient. 

We can also do a [likelihood ratio test (LRT)](https://en.wikipedia.org/wiki/Likelihood-ratio_test) as these models are nested within each other (all parameters in `fit1` are also present in `fit2`, so `fit1` is nested in `fit2`). The test statistic is $\Lambda = -2(l_2-l_1)$ where $l_2$ is the log-likelihood of `fit2`, and $l_1$ is the log-likelihood of `fit1` (nested model). The degrees of freedom are the same as in the chi-squared test. We can note that models must be fit using maximum likelihood (`ML` argument) to perform the LRT.

```{r mem-lrt1}
anova(fit1, fit2)
```

Again, we have a large p-value and come to the same conclusion as in the Wald test. 

Now that we have come to the conclusion that the time trends are the same among groups, the same methods could be used to test the hypothesis that there is no time effect ($H_0: \beta_2 = \beta_3 = \beta_4 = 0$) or the hypothesis that there is no difference in mean outcome by gender ($H_0: \beta_1 = 0$).

### Population Means {#mem-linear-population}

Using the asymptotic results of our MLE for $\beta$, we can estimate the population means for different subgroups in the data, and/or at different time points. 

For example, suppose we would like to know the mean distance at age 14 for males in the study, i.e. we want to estimate $\mu = \beta_0 + \beta_1 + \beta_4$. We can define a vector

$$
L = [1,1,0,0,1]
$$
to obtain the estimate

$$
\widehat{\mu}  = L\widehat{\beta} = \widehat{\beta_0} + \widehat{\beta_1} + \widehat{\beta_4}
$$

along with its standard error

$$
se(\widehat{\mu}) = \sqrt{L\widehat{Var}(\widehat{\beta})L^T}.
$$

The code to obtain these estimates is as follows:

```{r mem-linear-popmean}
betahat <- fit1$coef #get estimated betas from model 1
varbeta <- fit1$varBeta #get estimated variance covariance matrix from model 1

L <- matrix(c(1,1,0,0,1), nrow =1) #set up row vector L

muhat <- L%*%betahat #calculate estimated mean
se <- sqrt(L%*%varbeta%*%t(L)) #calculated estimated variance

muhat
se
```

With these quantities, we can also construct a 95% confidence interval for the mean as $\widehat{\mu} \pm 1.960*se(\widehat{\mu})$

```{r mem-linear-popCI}
CI.l <- muhat - 1.960*se #calculate lower CI bound
CI.u <- muhat + 1.960*se #calculate upper CI bound

print(paste("(", round(CI.l,3), ", ", round(CI.u,3), ")", sep = "")) #output the CI 

```


### Selecting a Covariance Structure {#mem-linear-cov}

In the previous examples, for illustrative purposes we assumed an independent covariance structure for. It is likely that in practice, the repeated measures within an individual are in fact correlated and thus we need to perform some tests to select the appropriate structure. 

To select a correlation structure, we use the `REML` fit instead of `ML` when fitting our models, as maximum likelihood estimation is biased for our covariance parameters. Our goal is to choose the simplest correlation structure while maintaining an adequate model fit. 

Some correlation structures are nested in each other, and we can perform a likelihood ratio test to assess the adequacy of the correlation structure. For example, the exchangeable correlation structure is nested within the unstructured covariance structure. As such, we can perform a hypothesis test of 

$$
H_0: \text{The simpler correlation structure (exchangeable) fits as well as the more complex structure (unstructured)}
$$

To do this test, we fit our model (we will do the main effects model here) using restricted maximum likelihood estimation and perform the LRT.

```{r mem-linear-corrstruct1}
fit1.unstructured <-  gls(distance ~ factor(gender)+factor(age), data = dental.long, 
            method = "REML", 
            corr = corSymm(form= ~1 | subject)) #subject is the variable indicating 
                            #repeated measures. Unstructured corr

fit1.exchangeable <- gls(distance ~ factor(gender)+factor(age), data = dental.long, 
            method = "REML", 
            corr = corCompSymm(form= ~1 | subject)) #subject is the variable indicating 
                            #repeated measures. Exchangeable corr
anova(fit1.unstructured, fit1.exchangeable)

```

We have a large p-value and thus fail to reject the null hypothesis. That is, we will use the exchangeable correlation structure as it is simpler and fits as well. 

For non-nested correlation structures like AR(1) and exchangeable, we can use AIC or BIC to assess fit, where a smaller AIC/BIC indicates a better fit. 


### Final Note: Model Fitting Procedure {#mem-linear-final}
When fitting models, it is usually recommended to first focus on the time trend of the response (should we have it continuous or discrete? Do we need higher-order terms?). Then, we look for the appropriate covariance structure and then consider variable selection. This can be done iteratively until a final model is chosen based on the appropriate statistical tests.

## Linear Mixed Effect Models {#mem-linearmixed}

Linear mixed effects models model both the population average (similar to the linear model in the previous section) along with subject-specific trends. We do this by including subject-specific regression coefficients into our model, as 

$$
Y_{ij} = \bf{x}_{ij}^T\bf{\beta} + \bf{z}_{ij}^T\bf{b}_{i} + \epsilon_{ij}
$$

where

* $Y_{ij}$ is the response of individual $i$ at time $j$,
* $\bf{x}_{ij}$ is a $p \times 1$ covariate vector for the fixed effects,
* $\bf{\beta}$ is the vector of parameters for the fixed effects,
* $\bf{z}_{ij}$ is a $q \times 1$ covariate vector for the random effects,
* $\bf{b}_{ij}$ is a vector of parameters for the random effects, and
* $\epsilon_{ij}$ is the random error associated with individual $i$ at time $j$.

Typically we assume the covariate vector for the random effects is a subset of the fixed effects.

We can write this in matrix form as

$$
\bf{Y}_i = \bf{X}_i \bf{\beta} + \bf{Z}_i\bf{b}_i + \bf{\epsilon}_i
$$

where 

$$
\bf{Y}_i = \begin{bmatrix}
Y_{i1} \\
Y_{i2} \\
\vdots \\
Y_{i,k_i}\\
\end{bmatrix} ,    \bf{X}_i = \begin{bmatrix}
\bf{x}_{i1}^T\\
\bf{x}_{i2}^T \\
\vdots \\
\bf{x}_{i, k_i}^T\\
\end{bmatrix}, \bf{Z}_i = \begin{bmatrix}
\bf{z}_{i1}^T\\
\bf{z}_{i2}^T \\
\vdots \\
\bf{z}_{i, k_i}^T\\
\end{bmatrix}, \text{     and } \bf{\epsilon}_i = \begin{bmatrix}
\epsilon_{i1}^T\\
\epsilon_{i2}^T\\
\vdots\\
\epsilon_{i,k_i}^T
\end{bmatrix}.
$$


