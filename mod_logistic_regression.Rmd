
# Introduction to Logistic Regression {#logreg}

*Author: Grace Tompkins, Yuliang Shi*

*Last Updated: May 6, 2022*

--- 
```{r cache-chunk, include = F}
knitr::opts_chunk$set(cache = T)
```

```{r functions-chunk, echo = F, warning = F, message = F}
source("common_functions.R")
# require("styler")
# styler::style_file()
```


## Introduction {#logreg-intro}

Researchers are often interested in describing the association between a [binary](https://www.statisticshowto.com/binary-variable-2/) outcome (an outcome that can only take on two values such as true of false, yes or no) and other factors of interest. For example, we may be interested in seeing if the presence of a disease (whether an individual has the disease or not) is related to age, lifestyle habits such as smoking or drinking, measures of socio-economic status, and whether there exists a family history of the disease. We could also quantify the association between the presence of certain species of frogs in ponds and the water temperature, pond depth, and vegetation present. We can also predict the probability of the outcome of interest occurring given the factors we've measured. We can answer these, and many other research questions involving binary responses/outcomes, by employing [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression). 

Logistic regression allows us to quantify the relationship between an outcome and other factors similar to traditional linear regression. However,  traditional linear regression is limited to continuous outcomes and is thus not suitable for modelling binary outcomes. This is because the normality assumption of the outcome will be violated. 

In this section, we will focus on analyzing data involving binary outcomes using logistic regression models in R. 

## List of R Packages {#logreg-rpackages}

In this chapter, we will be using the packages `r cran_link("catdata")`, `r cran_link("LogisticDx")`, and `cran_link("car")`.


```{r long-library, warning=FALSE, message=FALSE}
library(catdata) # load the required packages
library(LogisticDx)
library(car)
```


## Motivating Example {#logreg-example}

Let us look at an example using the `heart` data set from the `r cran_link("catdata")` package in R. This data set contains a retrospective sample of 462 males between ages 15 and 64 in South Africa where the risk of heart disease is considered high. We have data on whether or not the subject has coronary heart disease (CHD) (`y` = 1 indicates subject has CHD), measurements of systolic blood pressure (`sbp`), cumulative tobacco use (`tobacco`), low density lipoprotein cholesterol (`ldl`), adiposity (`adiposity`), whether or not the subject has a family history of heart disease (`famhist` = 1 indicates family history), measures of type-A behavior (`typea`), a measure of obesity (`obestiy`),  current alcohol consumption (`alcohol`), and the subject's age (`age`). 

```{r logistic-library, warning=FALSE, message=FALSE}
data("heart") # load the data set heart from catdata package

# convert to data frame
heart <- data.frame(heart)

# view first 6 rows of data
head(heart)
```

For illustrative purposes, we will convert `age` into a categorical variable to have a multi-level categorical variable in our analysis. We will convert our age into 10-year age categories: 1: 15 to 24, 2: 25 to 34, 3: 35 to 44, 4: 45 to 54, 5: 55 to 64. We emphasize that this decision is just to show readers how to work with categorical factors. We convert our variable into a factor using the following code:
```{r logistic-conver, warning = F, message = F}
#make a copy of age
heart$age_f <- heart$age

# overwrite it, making groups by age
heart$age_f[heart$age %in% 15:24] <- 1
heart$age_f[heart$age %in% 25:34] <- 2
heart$age_f[heart$age %in% 35:44] <- 3
heart$age_f[heart$age %in% 45:54] <- 4
heart$age_f[heart$age %in% 55:64] <- 5
```


From this data set, we'd like to see if there is a relationship between CHD diagnosis and tobacco use. We wish to control for other factors in our analysis, which we can do using a logistic regression model. 


## Assumptions {#logreg-assumptions}

There are three main assumptions that are required when employing logistic regression. We assume that

- the outcome is binary, i.e. either 0 or 1, true or false,

- the observations are independent, meaning there is no correlation between observations, and

- there is no [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) among explanatory variables. 

Multicollinearity (sometimes referred to as collinearity) occurs when two or more explanatory variables are highly correlated, such that they do not provide unique or independent information in the regression model. If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the model. One way to detect multicollinearity is to calculate the [variation inflation factor (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor) for each covariate in a model.  The general guideline is that a VIF larger than 5 or 10 indicates that the model has problems estimating the coefficient possibly due to multicollinearity. We will show in Section \@ref(logreg-diagnostics) how to use R functions to calculate the VIF after model building. 

We note that our methodology will be particularly sensitive to these assumptions when sample sizes are small. When collecting data, we also want to ensure that the sample is representative of the population of interest to answer the research question(s).

## Notation and Model Specification {#logreg-notation}

Logistic regression relates a binary outcome $\bm{Y_i}$ to a set of covariates $\bm{x_i}$ through the mean, as
$$
g(\mu_i) = \bm{x}_i^T\bm{\beta} 
$$
where $Y_i$ is the binary outcome for individual $i$, $bm{x}_i$ is a $p \times 1$ vector of covariates for individual $i$, and $g(\cdot)$ is a [link function](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function). For logistic regression, we use the [logit link](https://en.wikipedia.org/wiki/Logit), meaning we express our model as

$$
\text{log} \left[\frac{P(Y_i=1|\bm{x}_i)}{P(Y_i=0|\bm{x}_i)} \right] = \text{log} \left[\frac{\pi_i}{1-\pi_i} \right]=\bm{x_i}^T\bm{\beta}
$$
That is, we model the natural logarithm of the ratio of the probability that $Y_i$ is one versus zero. We denote the probability that $Y_i$ is one given covariates $\bm{x}_i$ as $\pi_i$. We refer to the quantity $\text{log} \left[\frac{\pi_i}{1-\pi_i} \right]$ as the **log-odds** (the logarithm of the [odds](https://en.wikipedia.org/wiki/Odds)) where the odds is the ratio of the probability an event occurring ($P(Y_i = 1)$) versus not occurring ($P(Y_i = 0)$). For example, in a roll of a standard six-sided die, the odds of rolling a three is 
$$
\text{Odds} = \frac{P(\text{3 is rolled})}{P(\text{3 is not rolled})} = \frac{(1/6)}{(5/6)} = 1/5 = 0.2
$$
We emphasize that the odds are **not** equal to the probability of an event happening. The probability of rolling a three on a standard die is $1/6 = 0.167$, which is not equal to the odds, however the two quantities are related by the definition of the odds.  

Notice that in our model that there is no random error term. This may be surprising as we typically see a random error term in linear regression models, however because we are modelling the mean of the outcome a random error term is not needed. 

Our primary interest is in estimating the coefficients $\bm{\beta}$ in our model. These coefficients can be interpreted as log odds ratio of the outcome for a one unit change in the corresponding covariate.  For example, if we consider a discrete covariate $x_1$ representing disease presence, we would interpret $\beta_1$ as the estimated log odds ratio of the outcome $Y$ for those with the disease versus without, controlling for other covariates in the model. For continuous covariates such as age, we would interpret the coefficient as the estimated log odds ratio associated with a one year increase in age, controlling for other covariates. We will show examples of the model interpretation for real data examples in the following sections, with a focus on building, evaluating, and interpreting models in R. 

## Data Pre-processing {#logreg-R-prepocessing}

Before building logistic regression models in R, we need to pre-process or "clean" the data. The first thing we can do is ensure the covariates in our data set are the correct type (continuous, categorical, etc) so that the model will be appropriately fit. 

Recall that we'd like to see if the outcome `y` (representing CHD presence) is associated with tobacco use while controlling for other factors such as blood pressure, cholesterol levels, adiposity, family history, type-a, obesity, alcohol, and age.  

The variables `sbp`, `tobacco`, `adiposity`, `obesity`, and `alcohol`, are continuous covariates and thus do not need to be specified as such. The variable `famhist` is a binary variable, and `age.f` is a categorical variable so these need to be specified as categorical variables or "factors" in R. We also need to convert the data set to a data frame. We do so with the following:

```{r logistic-R-cleanup, warning=FALSE}
# specify categorical variables as factors
heart$famhist_f <- as.factor(heart$famhist)
heart$age_f <- as.factor(heart$age_f)
```

Other things we may need to deal with in the data cleaning stage include missing data and duplicated responses. In our case, we do not have to deal with any of these issues and will continue with our analysis. 

## Fitting Logistic Regression Models {#logreg-R-modelfit}

To fit a logistic regression model in R, we fit a generalized linear model using the `glm()` function and specify a logistic link function by using the `family=binomial(link = "logit")"` argument. For example, we can build the main-effects only logistic regression model considering all covariates previously described by:
```{r logistic-R-firstfit, warning=FALSE, cache=TRUE}
#build the logistic model
heart_modelmaineffects <- glm(y ~ sbp + tobacco + ldl + adiposity + famhist_f + 
                         typea + obesity + alcohol + age_f, 
                            family=binomial(link = "logit"), data=heart)

#show the output
summary(heart_modelmaineffects)
```

Calling `summary()` on the model fit provides us with various estimates for the regression coefficients ("Estimate"), standard errors ("Std. Error"), and the associated Wald test statistic ("z value") and $p$-value ("Pr(>|z|)") for the null hypothesis that the corresponding coefficient is equal to zero.

While we could interpret this model and perform hypothesis tests, we must consider model selection to find the most appropriate model to answer our research questions. 

## Model Selection {#logreg-R-modelselection}

We aim to find a [parsimonious model](https://www.statisticshowto.com/parsimonious-model/#:~:text=Parsimonious%20models%20are%20simple%20models,called%20lex%20parsimoniae%20in%20Latin)) that addresses our research questions. That is, we aim to find the simplest model that explains the relationship between the outcome (whether the subject has CHD) and covariate(s) of interest.


### Likelihood Ratio Tests {#logreg-R-modelselection-LRT}
One of the most common ways to compare models against each other is through the [likelihood ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test) (LRT). For the likelihood ratio test, we can compare [nested](https://www.statology.org/nested-model/#:~:text=A%20nested%20model%20is%20simply,variables%20in%20another%20regression%20model.) models. That is, we can compare a full model to a nested model that contains a subset of variables that appear in the full model (and no other variables or transformations). Likelihood ratio tests tend to be the preferred method for building logistic regression models where we want to draw claims and perform hypothesis tests. 

We typically start model selection with a full main-effects only model and look at the least significant covariates in the full model. For example, `alcohol`, `adiposity` and `sbp` have large $p$-values for the Wald test that $\beta_{sbp} = 0$ and $\beta_{adiposity} = 0$ and $\beta_{alcohol} = 0$. We can see if these covariates are necessary in the model by testing the full model against one that does not contain those two covariates by a LRT. We do so in R by first fitting the new model, obtaining the residual deviance from both models we are comparing, and performing the LRT using the degrees of freedom (df) equal to the difference in the number of covariates between the two models (here, we removed 3 covariates and thus we have 3 degrees of freedom). The null hypothesis here is that `heart_model2` is adequate compared to `heart_modelfull`. 

```{r logistic-R-lrt1, warning=FALSE, message=FALSE}
# fit the nested model without sbp, alcohol, or adiposity
heart_model2 <- glm(y ~ tobacco + ldl + famhist_f + typea + 
                      obesity + age_f, 
                    family=binomial(link = "logit"), data=heart)

# perform the LRT 
anova(heart_model2, heart_modelmaineffects, test = "LRT")
```
We have a large $p$-value here, indicating that we do NOT reject the null hypothesis. That is, we are comfortable moving forward with the simpler model. Let's take a look at the model summary:
```{r logistic-F-fit2summary}
summary(heart_model2)
```
We notice that as we add and remove covariates, the estimates of our coefficients, standard errors, test statistics, and $p$-values will change. We see that  `obestiy` does not appear to be significant in the model. We can see if this variable are necessary again by performing an LRT against our second model:

```{r logistic-R-lrt2, warning=FALSE, message=FALSE}
# fit the nested model without obesity
heart_model3 <- glm(y ~ tobacco + ldl + famhist_f + typea + age_f, 
                    family=binomial(link = "logit"), data=heart)

# perform the LRT 
anova(heart_model3, heart_model2, test = "LRT")

```
We have a moderate $p$-value here, indicating that we do NOT reject the null hypothesis. That is, we are comfortable moving forward with the simpler model (`heart_model3`). Let's take a look at the model summary:
```{r logistic-F-fit3summary}
summary(heart_model3)
```
All of the remaining covariates in our model are significantly significant. While we could stop the model selection procedure here, we should also consider interactions and higher-order terms in our model selection. 

It is possible that those with a family history of heart disease may have different cholesterol levels than those who do not. We can consider adding in this interaction term to account for the potential difference in cholesterol levels by family history, and its impact on chronic heart disease diagnosis. We further fit another model with the interaction and perform an LRT against the model without the interaction by:
```{r logistic-R-lrt3, warning=FALSE, message=FALSE}
# fit the nested model with an interaction term
heart_model4 <- glm(y ~ tobacco + ldl + famhist_f + typea + age_f + ldl*famhist_f, 
                    family=binomial(link = "logit"), data=heart)

#show the output

# perform the LRT 
anova(heart_model3, heart_model4, test = "LRT")

```

We have a small $p$-value here, indicating that we reject the null hypothesis that the simpler model fits as well as the larger model. That is, we should include the interaction term in our model. Let's look at the summary of this model:
```{r logistic-R-summarymod4, warning=FALSE, message=FALSE}
summary(heart_model4)

```
We notice that the main effects of `ldl` and `famhist_f` are now insignificant. However, as we have the interaction term in the model, we tend to want to keep the main effects in the model as well to interpret. We note that there is a higher chance that the interaction will be significant if the main effects are as well. 


We also note that for categorical variables, we do not test if individual levels of the covariate should be included. That is, if one level of `age_f` was not significant, we do not remove that one level. We would need to do an LRT to see if the entire covariate of `age_f` was necessary. We can do so by the following:
```{r logistic-R-cat}
# fit the model without age_f
heart_model5 <- glm(y ~ tobacco + ldl + famhist_f + typea + ldl*famhist_f, 
                    family=binomial(link = "logit"), data=heart)

# perform the LRT 
anova(heart_model4, heart_model5, test = "LRT")
```
We have a very small $p$-value, indicating that we reject the null hypothesis that the simpler model is better. That is, `age_f` is necessary in our model and we should use `heart_model4` as our final model!


### AIC and BIC

For nested or non-nested models, we can perform model selection by either [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) or [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion) where a smaller value represents a better fit. Although both AIC and BIC are similar, research has shown that each are appropriate for different tasks, as discussed [here](https://en.wikipedia.org/wiki/Akaike_information_criterion#Comparison_with_BIC). For this problem, AIC and BIC are comparable and either are appropriate for use. 

We will compare the final model selected by LRTs to a built-in stepwise model selection procedure in R using the [`step()`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/step) function. The `step()` function evaluates the model on the [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion), where a smaller value represents a better fit. To do so in R, we use this function on the full model and perform a backward selection procedure by:
```{R logistic-R-step, warning = F, message = F}
#fit a full model, including the interaction
heart_full <- glm(y ~ sbp + tobacco + ldl + adiposity + famhist_f + 
                         typea + obesity + alcohol + age_f + ldl*famhist_f, 
                            family=binomial(link = "logit"), data=heart)


step(heart_full, direction = c("backward"))
```
The final model contains `tobacco`, `ldl`, `famhist_f`, `typea`, `age_f`, and the interaction between `ldl` and `famhist_f` as covariates. The model chosen by the `step()` function in this case is exactly the same as the one we obtained by the LRT. However, depending on the order we perform the likelihood ratio tests, or the direction of the stepwise algorithm, we can obtain different model results. Likelihood ratio tests tend to be preferred over AIC based algorithms for building logistic regression models where we want to draw claims and perform hypothesis tests while AIC based algorithms tend to be preferred for forecasting problems. As such, we will continue our analysis with the model chosen by our LRTs. 


We can also call the `AIC()` function on models and compare them manually. For illustrative purposes, lets compare a model that contains `tobacco`, `famhist_f`, `typea`, and `age_f` to one that contains `tobacco`, `ldl`, `typea`, and `age_f`. We note that these models are not nested, because neither model contains variables that are a subset of each other (the first model has `fam_hist` while the second does not, and the second model has `ldl` while the first does not.) Let's compare the fits of these models:
```{r logistic-R-aicbic}
heart_modelA <- glm(y ~ tobacco +  famhist_f + typea + age_f, 
                    family=binomial(link = "logit"), data=heart)

heart_modelB <- glm(y ~ tobacco + ldl + typea + age_f, 
                    family=binomial(link = "logit"), data=heart)

AIC(heart_modelA)
AIC(heart_modelB)
```
We see that the first model has a smaller AIC than the second model, indicating a better model fit of the two options. Calling `BIC()` will yield similar results. 


## Model Diagnostics {#logreg-diagnostics}

Before interpreting the chosen model, we must assess the model fit. We can do so by plotting the deviance residuals to give an idea of the model fit. We can do so in R by:
```{r logistic-md-plots, fig.cap="Plot of Residuals for Logistic Regression Model"}
# Plot the residuals
plot(residuals(heart_model4, type = "pearson"), ylab = "Pearson Residuals")
```

We notice that there are two subjects with high residual values. To identify them, we can use the following code;
```{r logistic-md-identifyresid}
# sort residuals largest to smallest and select the first two
sort(residuals(heart_model4, type = "pearson"), decreasing = T)[1:2]
```

Subjects 261 and 21 have high deviance residuals. To see if these are [influential observations](https://en.wikipedia.org/wiki/Influential_observation#:~:text=In%20statistics%2C%20an%20influential%20observation,effect%20on%20the%20parameter%20estimates.), we can refit the logistic regression model without these observations. If the estimates of our model change greatly, then we should remove these two observations as they may affect inference and predictions made with the logistic regression model.

Let's make a second data set without the 261st observation and see if the results of the model change. 
```{r logistic-md-remove261}
heart2 <- heart[-261,] # removing the 261st observation

#fit the model using heart2
heart_model4_2 <- glm(y ~ tobacco + ldl + famhist_f + typea + age_f + ldl*famhist_f, 
                    family=binomial(link = "logit"), data=heart2) #use heart2

summary(heart_model4_2) #model summary excluding id 261
summary(heart_model4) #original model summary
```
We do see that the estimated coefficients and standard errors change after removing the observation. In particular, the estimated regression coefficients and standard errors of the `age_f` variable changed greatly. This shows that observation 261 is an influential observation, and should be removed. 

We can do the same procedure for removing observation 21:
```{r logistic-md-remove21}
heart3 <- heart[-c(21, 261),] # removing the 21st (and 261st from prev removal) 
# observation

#fit the model using heart3
heart_model4_3 <- glm(y ~ tobacco + ldl + famhist_f + typea + age_f + ldl*famhist_f, 
                    family=binomial(link = "logit"), data=heart3) #use heart2

summary(heart_model4_3) #model summary excluding id 261
summary(heart_model4) #original model summary
```
Again we see that the coefficients change significantly and deem the 21st observation to be influential. Thus, we continue our analysis without observation 21 and 261.


```{r logistic-md-plots2, fig.cap="Plot of Residuals for Logistic Regression Model\\ with Influential Observation Removed"}
# Plot the residuals
plot(residuals(heart_model4_3, type = "pearson"), ylab = "Pearson Residuals")
```

Now, we see that most residual values fall between (-2, 2) (with no values beyond $\pm$3), which indicates a proper model fit. 

We notice that when we look at the model summary, some covariates have very large estimated standard errors and are no longer significant, indicating that after removing the influential observations we should re-do our model fitting. 
```{R logistic-R-stepnoinfluential, warning = F, message = F}
#fit a full model, including the interaction
heart_full2 <- glm(y ~ sbp + tobacco + ldl + adiposity + famhist_f + 
                         typea + obesity + alcohol + age_f + ldl*famhist_f, 
                            family=binomial(link = "logit"), data=heart3)


step(heart_full2, direction = "both", trace = 0) #trace = 0 means don't print
# every step
```

Let's see the summary of the model:
```{r logistic-md-modelfit2}
# fit a new model
heart_model6 <- glm(y ~ tobacco + ldl + famhist_f + 
                         typea + age_f + ldl*famhist_f , 
                            family=binomial(link = "logit"), data=heart3)
summary(heart_model6)
```
Although the stepwise procedure (and an LRT) will tell us otherwise, we should remove the age_f variable due to the large, insignificant estimates. It is likely that for this sample, `age_f` is not adding any value to our model. We refit this model without `age_f`:

```{r logistic-md-modelfit3}
# fit a new model
heart_model7 <- glm(y ~ tobacco + ldl + famhist_f + 
                         typea + ldl*famhist_f , 
                            family=binomial(link = "logit"), data=heart3)
summary(heart_model7)
```
and see estimates with less extreme values and variances. When we plot the residuals:

```{r logistic-md-plots3, fig.cap="Plot of Residuals for Second \\Logistic Regression Modelwith Influential Observation Removed"}
# Plot the residuals
plot(residuals(heart_model7, type = "pearson"), ylab = "Pearson Residuals")
```

We see that the residuals are behaving as expected with no extreme values. 

We should also check for multicollinearity in our model. We can do so by using the `vif()` function from the `r cran_link("car")` package. We can do so by the following R code:
```{r logistic-md-collinearity}
vif(heart_model7)
```

We see that we only have low-moderate variance inflation factors (VIFs), indicating that multicollinearity is not an issue in this model. We typically are concerned about multicollinearity when VIF values are above 10. 

## Model Interpretation and Hypothesis Testing {#logreg-ht}

### Odds Ratios {#logreg-ht-or}

As previously mentioned in Section \@ref(logreg-notation), we interpret each coefficients as the log odds ratio of the outcome for a one unit change in the corresponding covariate, controlling for the other covariates in the model. That means, to obtain estimates of the odds ratio, we take the exponential (`exp()` in R) of the coefficient. The estimates of the standard error for the coefficients (log-odds) will be useful for hypothesis testing and constructing confidence intervals.

Let's look again at the model we chose from the selection procedure:
```{r logistic-MIHT-summary}
summary(heart_model7)
```
In mathematical notation, we can write this model as

$$
\begin{aligned}
\text{log} \left[\frac{\pi_i}{1-\pi_i} \right]= \beta_0 + &\beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \beta_4x_4 + \beta_5x_2*x_3
\end{aligned}
$$
where $pi_i$ is the probability of having CHD, $x_1$ is the measurement of cumulative tobacco use, $x_2$ is the ldl cholesterol measurement, $x_3$ is an indicator for family history, and $x_4$ is the type a measurement.

We are provided with the estimates and standard errors of the log odds ratios, but typically want to interpret and communicate our findings on the scale of odd ratios. To obtain an estimate of the odds ratio for a given covariate, we simply exponentiate the coefficient. For example, the `tobacco` covariate's estimated coefficient ($\widehat{\beta_1}$) is `r paste(round(summary(heart_model7)$coef[2,1],3))`, meaning we estimate that a one unit increase in `tobacco` is associate with a **log odds ratio** of chronic heart disease equal to `r paste(round(summary(heart_model7)$coef[2,1],3))`, controlling for the other factors in the model. Alternatively, we can say that a one unit increase in `tobacco` is associate with an **odds ratio** of chronic heart disease equal to $exp($ `r paste(round(summary(heart_model7)$coef[2,1],3))` $)$ = `r paste(round(exp(summary(heart_model7)$coef[2,1]),3))``, controlling for other factors. 

[Confidence intervals](https://en.wikipedia.org/wiki/Confidence_interval) (CIs) are useful in communicating the uncertainty in our estimates and are typically presented along with our estimate. Confidence intervals are calculated on the log odds ratio scale, and then exponentiated to find the confidence interval for the odds ratio. We calculate a 95% confidence interval for a log odds ratio as:

$$
\widehat{\beta} \pm 1.96\times \widehat{se}(\widehat{\beta})
$$
where $\widehat{se}(\widehat{\beta})$ is the estimated standard error of the regression coefficient. For example, a 95% confidence interval for the log odds ratio of `tobacco` is
$$
0.138 \pm 1.960 \times 0.025 = (0.089, 0.187).
$$
Then, to find the 95% CI for the odds ratio, we exponentiate both sides of the CI as:
$$
(\exp(0.089), \exp(0.187)) = (1.093, 1.206)
$$
So, we estimate the odds ratio to be `r paste(round(exp(summary(heart_model7)$coef[2,1]),3))` (95% CI: (1.093, 1.206)) controlling for the other factors in the model. As this 95% CI does not contain the value of 1 in it, we say that we are 95% confident that higher tobacco use is associated with an increased odds of developing CHD. 

For the odds ratios and confidence intervals of the regression coefficients individually, one can also call the `confint.default()` function

```{r logistic-MIHT-confint}
logORs <- cbind(coef(heart_model7), confint.default(heart_model7))
colnames(logORs) <- c("logOR", "Lower", "Upper")
logORs
```
and exponentiate it to get the ORs:
```{r logistic-MIHT-confint2}
ORs <- exp(logORs)
colnames(ORs) <- c("Odds Ratio", "Lower", "Upper")
ORs

```


For another example, let's look at estimating the odds ratio of CHD for a one unit increase in `ldl` among those with a family history of CHD, controlling for the other factors. For more complex estimates where we may be interested in combinations of covariates, it can be useful to create a table to determine what regression coefficients we want to use.  


Recall the model 
$$
\begin{aligned}
\text{log} \left[\frac{\pi_i}{1-\pi_i} \right]= \beta_0 + &\beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \beta_4x_4 + \beta_5x_2x_3.
\end{aligned}
$$
and recall $x_2$ represents `ldl` and $x_3$ represents `famhist_f`.

We wish to estimate the odds ratio of CHD for a one unit increase in `ldl`, which is the same as looking at $x_2$ = 1 versus $x_2 = 0$ (or $x_2 = 2$ versus $x_2 = 1$, and so on, however 1 versus 0 is the simplest example). We also are interested in only those with a family history of CHD, represented by $x_3 = 1$. All of the other covariates are held constant. So, we are comparing

$$
\begin{aligned}
\beta_0 + &\beta_1x_1 + \beta_2(1) + \beta_3(1) + \beta_4x_4 + \beta_5(1)(1)
\end{aligned}
$$
to
$$
\begin{aligned}
\beta_0 + &\beta_1x_1 + \beta_2(0) + \beta_3(1) + \beta_4x_4 + \beta_5(0)(1)
\end{aligned}
$$
If we look at the difference of these equations, we have 
$$
\begin{aligned}
& \beta_0 + \beta_1x_1 + \beta_2(1) + \beta_3(1) + \beta_4x_4 + \beta_5(1)(1)\\
&-\beta_0 + \beta_1x_1 + \beta_2(0) + \beta_3(1) + \beta_4x_4 + \beta_5(0)(1)\\
\hline
& \qquad \qquad  \qquad \quad \beta_2 \qquad \quad \quad \quad  \qquad  \quad+\beta_5
\end{aligned}
$$
which shows that we should estimate and interpret $\beta_2 + \beta_5$ to answer this question. From the model output, we estimate the log odds ratio as $\widehat{\beta_2} + \widehat{\beta_5} = 0.099 + 0.301 = 0.400$. Then, the estimated odds ratio is $\exp(0.400) = 1.492$. So, we estimate that a one unit increase in low density lipoprotein cholesterol is associated with an odds ratio of CHD equal to 1.492, controlling for other factors. 

To estimate the confidence interval, we can use the `OR()` function from the `r cran_link("LogisticDx")` package where we specify `newdata` to be $(\beta_0,\beta_1,\beta_2,\beta_3,\beta_4,\beta_5)$ =(0,0,1,0,0,1), which represents $\beta_2 + \beta_5$. To obtain the CI for the odds ratio, we call
```{r logistic-MIHT-logisticdx}
OR(heart_model7, newdata = c(0,0,1,0,0,1))
```
which provides us with estimates of the odds ratio (OR), lowOR, and upOR where the 95% CI is (lowOR, upOR). The two lines are for two models: one with an intercept and without, though in this case the OR and CI's are the same. We can also find the CI manually by finding the estimated standard error of the $\widehat{\beta_2} + \widehat{\beta_5}$ manually.

### Probabilities {#logreg-ht-probabilities}

Perhaps we are interested in estimating or predicting the probability of the outcome instead of the odds ratio for given covariates. 

Suppose we are interested in the probability that a 25 year old with `spb` = 150, `tobacco` = 0, `ldl` = 6, `adiposity` = 24, no family history of CHD, `typea` = 60, `obesity` = 30, and `alcohol` = 10. Using the required information, we can obtain a prediction using the `predict()` function as:
```{r logistic-MIHT-prediction}

# make a vector of the new information as it would appear in the original 
# dataframe (excluding y). Use colnames(heart) to see the order of variables
newsubject <- data.frame(sbp = 150,
                         tobacco = 0,
                         ldl = 6,
                         adiposity = 24,
                         famhist = 0,
                         typea = 60,
                         obesity = 30,
                         alcohol = 10,
                         age = 25,
                         age_f = 2,
                         famhist_f = 1
                         )

newsubject$age_f <- as.factor(newsubject$age_f)
newsubject$famhist_f <- as.factor(newsubject$famhist_f)

predict(heart_model7, newdata = newsubject)

```

We estimate that $\frac{\log{\pi}}{1 - \pi} = -0.037$. So, to estimate the probability, we take the expit() of this estimate, or obtain
$$
\frac{\exp(-0.037)}{1 + \exp(-0.037)} = 0.491
$$
We estimate this hypothetical individual to have a 49.1% probability of having CHD given their covariates. 


